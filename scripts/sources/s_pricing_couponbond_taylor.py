#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# ---
# jupyter:
#   jupytext:
#     text_representation:
#       extension: .py
#       format_name: light
#       format_version: '1.4'
#       jupytext_version: 1.2.1
#   kernelspec:
#     display_name: Python 3
#     language: python
#     name: python3
# ---

# # s_pricing_couponbond_taylor [<img src="https://www.arpm.co/lab/icons/icon_permalink.png" width=30 height=30 style="display: inline;">](https://www.arpm.co/lab/redirect.php?code=s_pricing_couponbond_taylor&codeLang=Python)
# For details, see [here](https://www.arpm.co/lab/redirect.php?permalink=eb-coupon-bond-taylor-approx).

# +
import numpy as np
import pandas as pd
import scipy as sp
import matplotlib.pyplot as plt

from arpym.pricing import bond_value
from arpym.statistics import moments_mvou
from arpym.statistics import meancov_sp, saddle_point_quadn
from arpym.tools import histogram_sp, add_logo
# -

# ## [Input parameters](https://www.arpm.co/lab/redirect.php?permalink=s_pricing_couponbond_taylor-parameters)

j_ = 1000  # number of scenarios
deltat = 6  # time to the investment horizon
c = 0.04  # annualized coupons (percentage of the face value)
freq_paym = 1  # coupon payment frequency (years)
dy = 0.01  # numerical differentiation step (duration and convexity)

# ## [Step 0](https://www.arpm.co/lab/redirect.php?permalink=s_pricing_couponbond_taylor-implementation-step00): Upload data

# +
# import database generated by script s_projection_shadow_rates
path = '../../../databases/temporary-databases'

df = pd.read_csv(path + '/db_proj_scenarios_yield.csv', header=0)
tau = np.array(list(map(int, df.columns)))  # times to maturity
d_ = tau.shape[0]
df2 = pd.read_csv(path + '/db_proj_scenarios_yield_par.csv', header=0)
theta = np.array(df2['theta'].iloc[:d_ ** 2].values.reshape(d_, d_))
mu_mvou = np.array(df2['mu_mvou'].iloc[:d_])
sig2_mvou = np.array(df2['sig2_mvou'].iloc[:d_ ** 2].values.reshape(d_, d_))
df2 = pd.read_csv(path + '/db_proj_dates.csv', header=0, parse_dates=True)
t_m = df2.values
t_m = np.array(pd.to_datetime(df2.values.reshape(-1)), dtype='datetime64[D]')
m_ = t_m.shape[0]-1
deltat_m = np.busday_count(t_m[0], t_m[1])

if deltat > m_:
    print(" Projection doesn't have data until given horizon!!! Horizon lowered to ", m_)
    deltat = m_
# number of monitoring times
m_ = deltat
t_m = t_m[:m_+1]

j_m_, _ = df.shape
y_t_now_t_hor = np.array(df).reshape(j_, int(j_m_/j_), d_)
y_t_now_t_hor = y_t_now_t_hor[:j_, :m_+1, :]
y_t_now = y_t_now_t_hor[0, 0, :].reshape(1, -1)
# -

# ## [Step 1](https://www.arpm.co/lab/redirect.php?permalink=s_pricing_couponbond_taylor-implementation-step01): Record dates and coupons of the bond

# +
# number of coupons until bond maturity
tend = np.datetime64('2025-12-22')  # time of maturity
k_ = int(np.busday_count(t_m[0], tend)/(freq_paym*252))

# record dates
r = np.busday_offset(t_m[0], np.arange(1, k_+1)*int(freq_paym*252))
# coupons
coupon = c * freq_paym * np.ones(k_)
# -

# ## [Step 2](https://www.arpm.co/lab/redirect.php?permalink=s_pricing_couponbond_taylor-implementation-step02): Scenarios for bond value path

v_thor = np.array([bond_value(eval_t, coupon, r, 'y', y_t_now_t_hor[:, m, :],
                              tau_x=tau) for m, eval_t in enumerate(t_m)]).T

# ## [Step 3](https://www.arpm.co/lab/redirect.php?permalink=s_pricing_couponbond_taylor-implementation-step03): Scenarios for bond normalized P&L

y_t_now = y_t_now_t_hor[[0], 0, :]
v_t_now = bond_value(t_m[0], coupon, r, 'y', y_t_now, tau_x=tau)
v_t_now = v_thor[:, 0].reshape(-1, 1)
r_t_hor = (v_thor - v_t_now) / v_t_now

# ## [Step 4](https://www.arpm.co/lab/redirect.php?permalink=s_pricing_couponbond_taylor-implementation-step04): Scenario-probability expectations and standard deviations

# +
mu_r_t_hor = np.zeros(m_+1)
sig_r_t_hor = np.zeros(m_+1)

for m in range(len(t_m)):
    mu_r_t_hor[m], sig1 = meancov_sp(r_t_hor[:, m].reshape(-1, 1))
    sig_r_t_hor[m] = np.sqrt(sig1)
# -

# ## [Step 5](https://www.arpm.co/lab/redirect.php?permalink=s_pricing_couponbond_taylor-implementation-step05): Numerical greeks: yield, eff. key rate durations and convexities

# +
# numerical yield
v_t_now = v_t_now[0]
dt = 1/252  # numerical differentiation step (yield)
y_hat_tnow = (bond_value(np.busday_offset(t_m[0], dt*252), coupon, r, 'y',
                         y_t_now, tau_x=tau) - v_t_now) / (dt * v_t_now)

# effective key rate durations and convexities
dy_vec = dy * np.eye(d_)
dur_hat_d_tnow = np.zeros((d_, 1))
conv_hat_dl_tnow = np.zeros((d_, d_))

for d in np.arange(d_):
    dur_hat_d_tnow[d] = - ((bond_value(t_m[0], coupon, r, 'y',
                                       y_t_now + dy_vec[d, :], tau_x=tau) -
                            bond_value(t_m[0], coupon, r, 'y',
                                       y_t_now - dy_vec[d, :],
                                       tau_x=tau)).reshape(-1) /
                           (2 * dy * v_t_now))
    for l in np.arange(d_):
        conv_hat_dl_tnow[d, l] = ((bond_value(t_m[0], coupon, r, 'y',
                                              y_t_now + dy_vec[d, :] +
                                              dy_vec[l, :], tau_x=tau) -
                                   bond_value(t_m[0], coupon, r, 'y',
                                              y_t_now - dy_vec[d, :] +
                                              dy_vec[l, :], tau_x=tau) -
                                   bond_value(t_m[0], coupon, r, 'y',
                                              y_t_now + dy_vec[d, :] -
                                              dy_vec[l, :], tau_x=tau) +
                                   bond_value(t_m[0], coupon, r, 'y',
                                              y_t_now - dy_vec[d, :] -
                                              dy_vec[l, :],
                                              tau_x=tau)).reshape(-1) /
                                  (4 * v_t_now * dy ** 2))
# -

# ## [Step 6](https://www.arpm.co/lab/redirect.php?permalink=s_pricing_couponbond_taylor-implementation-step06): Moments of yield increment

# moments of yield increment
_, mu_y_dt, sig2_y_dt = moments_mvou(y_t_now.reshape(-1),
                                     [deltat*21],
                                     theta, mu_mvou, sig2_mvou)
mu_dy = mu_y_dt - y_t_now.reshape(-1)
sig2_dy = sig2_y_dt

# ## [Step 7](https://www.arpm.co/lab/redirect.php?permalink=s_pricing_couponbond_taylor-implementation-step07): Distribution of first order approximation

# +
# parameters of normal distribution
mu_r_deltat = y_hat_tnow * deltat/12 \
   - dur_hat_d_tnow.T @ mu_dy
sig2_r_deltat = dur_hat_d_tnow.T @ sig2_dy @ dur_hat_d_tnow

# normal pdf
k_ = 500
grid = np.linspace(mu_r_t_hor[-1] - 10*sig_r_t_hor[-1],
                   mu_r_t_hor[-1] + 10*sig_r_t_hor[-1], k_)
norm_pdf = sp.stats.norm.pdf(grid, mu_r_deltat,
                             np.sqrt(sig2_r_deltat[0, 0]))
# -

# ## [Step 8](https://www.arpm.co/lab/redirect.php?permalink=s_pricing_couponbond_taylor-implementation-step08): Effective duration and effective convexity

dur_hat_tnow = np.sum(dur_hat_d_tnow)
conv_hat_tnow = np.sum(conv_hat_dl_tnow)

# ## [Step 9](https://www.arpm.co/lab/redirect.php?permalink=s_pricing_couponbond_taylor-implementation-step09): Distribution of second order approximation

# +
# parameters of quadratic normal distribution
alpha = y_hat_tnow * deltat/12
beta = - dur_hat_d_tnow
gamma = 0.5 * np.eye(d_) * conv_hat_tnow / d_

# quad normal pdf via saddle point approximation
_, quadn_pdf = saddle_point_quadn(grid, alpha.reshape(-1), beta.reshape(-1),
                                  gamma, mu_dy, sig2_dy)
# -

# ## Plots

# +
plt.style.use('arpm')
lgrey = [0.8, 0.8, 0.8]  # light grey
dgrey = [0.4, 0.4, 0.4]  # dark grey
lblue = [0.27, 0.4, 0.9]  # light blue
orange = [0.94, 0.35, 0]  # orange
j_sel = 35  # selected MC simulations
scale = (np.busday_count(np.min(t_m), np.max(t_m))/252) * 0.05

# simulated path, mean and standard deviation

fig, axs = plt.subplots(1, 1)

t_axis = np.busday_count(t_m[0], t_m)/252
plt.plot(t_axis.reshape(-1, 1), r_t_hor[:j_sel, :].T, color=lgrey, lw=1)
plt.ylabel('Bond return')
plt.xlabel('horizon')
l2 = plt.plot(t_axis, mu_r_t_hor + sig_r_t_hor, color='r')
plt.plot(t_axis, mu_r_t_hor - sig_r_t_hor, color='r')
l1 = plt.plot(t_axis, mu_r_t_hor, color='g')
plt.grid(False)

# empirical pdf
p = np.ones(j_) / j_
y_hist, x_hist = histogram_sp(r_t_hor[:, -1], k_=10*np.log(j_))
y_hist = y_hist * scale  # adapt the hist height to the current xaxis scale
shift_y_hist = deltat/12 + y_hist

emp_pdf = plt.barh(x_hist, y_hist, left=t_axis[-1],
                   height=x_hist[1]-x_hist[0], facecolor=lgrey,
                   edgecolor=lgrey)

plt.plot(shift_y_hist, x_hist, color=dgrey, lw=1)
plt.plot([t_axis[-1], t_axis[-1]], [x_hist[0], x_hist[-1]], color=dgrey,
         lw=0.5)

# normal approximation
shift_norm_pdf = np.array([t_axis[-1]+i for i in (norm_pdf * scale)])
l3 = plt.plot(shift_norm_pdf, grid, color=lblue, lw=1)

# quadn approximation
shift_quadn_pdf = np.array([t_axis[-1]+i for i in (quadn_pdf * scale)])
l4 = plt.plot(shift_quadn_pdf, grid, color=orange, lw=1)

# axis lim
plt.xlim([np.min(t_axis), np.max([np.max(t_axis), np.max(shift_quadn_pdf),
          np.max(shift_norm_pdf),
          t_axis[0]+np.max(shift_y_hist)]) +
          np.min(np.diff(t_axis))])

# legend
plt.legend(handles=[l1[0], l2[0], emp_pdf[0], l3[0], l4[0]],
           labels=['mean', ' + / - st.deviation', 'emp. pdf',
                   'first order approx.', 'second order approx'])
plt.title('Taylor approximation P&L of a coupon bond')
add_logo(fig)
plt.tight_layout()
# -


